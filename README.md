

# Convolutional Neural Network from Scratch - MyLeNet

## ğŸ“Œ Opis projektu
Projekt ten zawiera implementacjÄ™ konwolucyjnej sieci neuronowej (**CNN**) od podstaw, przy uÅ¼yciu czystego **Pythona** i **NumPy**. Jako punkt odniesienia zastosowano klasycznÄ… architekturÄ™ **LeNet-5**, a jej efektywnoÅ›Ä‡ porÃ³wnano z implementacjÄ… w **Keras**. 

**Cel projektu:**  
âœ”ï¸ Zrozumienie dziaÅ‚ania CNN poprzez rÄ™cznÄ… implementacjÄ™  
âœ”ï¸ PorÃ³wnanie wynikÃ³w wersji "from scratch" i implementacji w Keras  
âœ”ï¸ Testowanie wydajnoÅ›ci i optymalizacji uczenia  

## ğŸ“‚ Struktura projektu

```bash
project_root/
â”œâ”€â”€ MyLeNet.py        # GÅ‚Ã³wna klasa definiujÄ…ca model MyLeNet
â”œâ”€â”€ MyANN.py          # Implementacja warstw sieci i optymalizatora SGD
â”œâ”€â”€ keras_imp.py      # Implementacja modelu CNN w Keras
â”œâ”€â”€ README.md         # Dokumentacja projektu
â””â”€â”€ docs/             # Wizualizacje wynikÃ³w 
```

### âš™ï¸ Wymagania

Przed uruchomieniem naleÅ¼y zainstalowaÄ‡ wymagane biblioteki:

```bash
pip install numpy tensorflow matplotlib scikit-learn
```

## ğŸ—ï¸ Implementacja MyLeNet
Model MyLeNet skÅ‚ada siÄ™ z nastÄ™pujÄ…cych warstw:

1. **Warstwa konwolucyjna (Conv1)**: 6 filtrÃ³w 5x5, aktywacja **Tanh**, pooling **Average Pool 2x2**
2. **Warstwa konwolucyjna (Conv2)**: 16 filtrÃ³w 5x5, aktywacja **Tanh**, pooling **Average Pool 2x2**
3. **Warstwa konwolucyjna (Conv3)**: 120 filtrÃ³w 5x5, aktywacja **Tanh**
4. **Warstwa gÄ™sta (Dense1)**: 480 â†’ 84 neuronÃ³w, aktywacja **Tanh**
5. **Warstwa gÄ™sta (Dense2)**: 84 â†’ 10 neuronÃ³w (Softmax)

Dane wejÅ›ciowe to obrazy **MNIST** w formacie **28x28 px**.

## ğŸš€ Uruchomienie MyLeNet

Trening modelu moÅ¼na uruchomiÄ‡ za pomocÄ…:

```python
from MyLeNet import MyLeNet1

network = MyLeNet1()
network.RunTraining(epochs=10, minibatch_size=128)
```

Po zakoÅ„czeniu treningu moÅ¼na przetestowaÄ‡ model:

```python
network.EvaluateTestSet()
network.EvaluateMyLeNet()
network.EvaluateConfusionMatrix()
```

## ğŸ”¬ Wyniki i porÃ³wnanie

| Model       | Test Accuracy | Test Loss | Czas treningu |
|------------|--------------|------------|---------------|
| **MyLeNet** | 90.6% | 0.369 | **168.84 sec** |
| **Keras** | 94.5% | 0.190 | **6.14 sec** |

- **MyLeNet osiÄ…ga solidne wyniki**, ale czas trenowania jest znacznie dÅ‚uÅ¼szy.
- **Keras pozwala na szybszÄ… konwergencjÄ™ modelu** dziÄ™ki zoptymalizowanym operacjom.

## ğŸ“Š Wykresy trenowania
PoniÅ¼ej przedstawiono wykresy **accuracy** i **loss** w funkcji epok dla obu implementacji:

### ğŸ”µ MyLeNet:
![image](https://github.com/user-attachments/assets/125d3d38-3918-4852-ab63-5199c46a8ddd)

### ğŸ”´ Keras:
![image](https://github.com/user-attachments/assets/e8eb6721-7fe9-4803-8897-4367058f3ea8)

## ğŸ“ˆ Macierze bÅ‚Ä™dÃ³w
Analiza bÅ‚Ä™dÃ³w pokazuje, Å¼e **MyLeNet** ma wiÄ™ksze trudnoÅ›ci z cyframi o podobnych ksztaÅ‚tach (np. **4 i 9**), podczas gdy model **Keras** popeÅ‚nia mniej bÅ‚Ä™dÃ³w.

### ğŸ”µ MyLeNet:
![image](https://github.com/user-attachments/assets/b1ff8e28-8036-4711-ab9b-2a8e84b8b553)

### ğŸ”´ Keras:
![image](https://github.com/user-attachments/assets/f709869b-f002-46b6-aa7e-b9f9e0770447)

## ğŸ“Œ Wnioski
âœ”ï¸ **Implementacja od podstaw** pozwala na lepsze zrozumienie dziaÅ‚ania CNN i algorytmÃ³w optymalizacji.  
âœ”ï¸ **Keras zapewnia wyÅ¼szÄ… dokÅ‚adnoÅ›Ä‡** i **znacznie krÃ³tszy czas treningu**.  
âœ”ï¸ **MoÅ¼liwe usprawnienia MyLeNet**:
  - **Batch Normalization** dla stabilizacji gradientÃ³w.
  - **Optymalizatory Adam / RMSprop** dla lepszego dostosowania learning rate.
  - **Wykorzystanie GPU (np. CuPy)** w celu przyspieszenia obliczeÅ„.

## ğŸ“œ Licencja
Projekt zostaÅ‚ stworzony w celach edukacyjnych. MoÅ¼esz swobodnie uÅ¼ywaÄ‡ i modyfikowaÄ‡ kod w ramach licencji **MIT**.

